{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/wavenet_vocoder.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WaveNet Vocoder Recipe Demonstration\n",
    "\n",
    "**Tomoki Hayashi**\n",
    "\n",
    "Department of Informatics, Nagoya University  \n",
    "Human Dataware Lab. Co., Ltd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Good afternoon, everyone.  \n",
    "This is Tomoki Hayashi, doctoral researcher @ Nagoya university.  \n",
    "From here, I will introduce my implementation of wavenet vocoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Access the notebook\n",
    "\n",
    "https://bit.ly/2kqDCEr\n",
    "\n",
    "<img src=https://user-images.githubusercontent.com/22779813/64497216-2154e180-d2e7-11e9-861a-7987af223fd0.png width=30%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If you have not yet open the notebook, please access the notebook from this colab URL.  \n",
    "You can also access from the my github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Environmental setup\n",
    "\n",
    "First, install dependecies (It takes several minutes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "First, let us setup the environment.  \n",
    "Please run the cell and please wait a minute until finish the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!apt-get install -qq -y bc tree\n",
    "!git clone https://github.com/kan-bayashi/PytorchWaveNetVocoder.git -b IS19TUTORIAL\n",
    "!git clone https://github.com/k2kobayashi/sprocket.git -b IS19TUTORIAL\n",
    "!cd sprocket && pip install -q .\n",
    "!cd PytorchWaveNetVocoder && pip install -q .\n",
    "!cd PytorchWaveNetVocoder && mkdir -p tools/venv/bin && touch tools/venv/bin/activate\n",
    "import sprocket, wavenet_vocoder  # check importable\n",
    "!echo \"Setup done!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is the PytorchWaveNetVocoder?\n",
    "\n",
    "Github: [kan-bayashi/PytorchWaveNetVocoder](https://github.com/kan-bayashi/PytorchWaveNetVocoder)  \n",
    "Samples: https://kan-bayashi.github.io/WaveNetVocoderSamples/\n",
    "\n",
    "- WaveNet vocoder implemention with pytorch\n",
    "- Support [kaldi](https://github.com/kaldi-asr/kaldi)-like recipes, easy to reproduce the results\n",
    "- Support [World](https://github.com/mmorise/World) features / mel-spectrogram based models\n",
    "- Support multi-gpu training / decoding\n",
    "- Support a noise shaping [[Tachibana+ 2018](https://ieeexplore.ieee.org/document/8461332)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "During the installation, let me introduce my implementation of wavenet vocoder, named pytorch wavenet vocoder.  \n",
    "You can check the code from my github and listen to the sample in the demo HP.  \n",
    "This repository provides the wavenet vocoder implementation using pytorch.  \n",
    "And it supports kaldi-like recipe, easy to reproduce the results.\n",
    "And support speech analyzer, world-based feature and mel-spectrogram based models.  \n",
    "So you can use this repository for not only voice conversion but also text-to-speech.  \n",
    "And it also supports multi-gpu training / decoding, and noise shaping techniques proposed by Tachibana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What it the kaldi-like recipe?\n",
    "\n",
    "Key features:\n",
    "- Prepared for each corpus (e.g. CMU Arctic, LJSpeech)\n",
    "- Consists of unified several stages  \n",
    "  (e.g. data preparation, feature extraction, and so on.)\n",
    "- Includes all procedures needed to reproduce the results\n",
    "- All of the recipes are stored in `egs/<corpus>/<type>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Maybe the most of you are familiar with kaldi, so here let me introduce kaldi-like recipe briefly.  \n",
    "Kaldi-like recipes are prepared for each corpus such as CMU Arctic, LJspeech.  \n",
    "These recipes consist of unified several stages, including data download, preparation, feature extraction, training and so on.  \n",
    "In other words, each recipe includes all of the procedures which needed to reproduce the results.  \n",
    "All of the recipes are stored in egs / the name of corpus / the type of recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Supported corpus:\n",
    "- [CMUArctic database](http://www.festvox.org/cmu_arctic/): `egs/arctic`, 16 kHz, English, Several speakers.\n",
    "- [LJ Speech database](https://keithito.com/LJ-Speech-Dataset/): `egs/ljspeech` 22.05 kHz, English, Single female speaker.\n",
    "- [M-AILABS speech database](http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/):`egs/m-ailabs-speech`: 16 kHz, various speakers\n",
    "\n",
    "About supported type, see detail in https://github.com/kan-bayashi/PytorchWaveNetVocoder/tree/master/egs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here is the list of supported corpus.\n",
    "CMU Arctic, LJSpeech, M-AILABS Speech dataset.\n",
    "About the supported recipe type, please check the URL.\n",
    "\n",
    "Of course you can add your own recipe using your own corpus.  \n",
    "After this demonstration, maybe you can understand how to apply your dataset to make recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Run the demo recipe\n",
    "\n",
    "Let us run the demo recipe `egs/arctic/sd-mini`.\n",
    "\n",
    "- Small version of `egs/arctic/sd`\n",
    "- Use subset of all of the utterances\n",
    "- **Cannot build a good model** but the flow is **the same**\n",
    "\n",
    "You can understand each stage within 30 minutes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "OK. Maybe you finished the installation.  \n",
    "Let us start the running the recipe.  \n",
    "In this demonstration, we use egs/arctic/sd-mini recipe, which is a small version of sd reicpe.  \n",
    "It uses subset of all of the utterances and unfortunately cannot build a good model but the flow is the completely same.  \n",
    "You can understand each stage within 30 minutes.  \n",
    "Let move on the recipe directory. Please run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "# move on the recipe directory\n",
    "import os\n",
    "os.chdir(\"./PytorchWaveNetVocoder/egs/arctic/sd-mini\")\n",
    "!echo $(pwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Files in the recipe are as follows:\n",
    "- `conf`: Directory including config files\n",
    "- `path.sh`: Script to set the environmental variables.\n",
    "- `run.sh`: Main script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "!tree -L 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "First, I will introduce the files included in the recipe directory.  \n",
    "Let me check the files.  \n",
    "Conf is the directory including config files, path.sh is the script ot set the environmental variable such as CUDA, and run.sh is the main script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`conf` includes f0 setting files whose name format is `<speaker_name>.f0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "!ls conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let me check the files in the conf directory.  \n",
    "conf includes f0 setting files whose name format is speaker_name.f0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<speaker_name>.f0` includes `min_f0 max_f0`.  \n",
    "These values are predecided by ourselve, so you can modify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat conf/slt.f0  # (minf0 maxf0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "conf includes f0 setting files whose name format is <speaker_name>.f0.  \n",
    "These files are used for world feature extraction.  \n",
    "And each <speaker_name>.f0 includes min_f0 and max_f0 values.  \n",
    "These values are predecided by ourselve, so you can modify them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "All of the hyperparameters are written in `run.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!head -n 69 run.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let us introduce these parameters in detail later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "All of the hyperparameters are written in the main script run.sh.\n",
    "Let me check the all of the hyperparameters.  \n",
    "These parameters can be changed from command line. of course you can directly edit the file.  \n",
    "Each of hyperparameters is explained in detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# (Optional) here you can add your command to check the file!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overview of the recipe\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/overview.png width=80%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "OK. Let us move ont the introduction of the flow of the recipe.  \n",
    "This figure is the overview of recipe.  \n",
    "The recipe consists of 7 stages from 0 to 6.  \n",
    "In stage 0, the corpus will be donwloaded from cloud and then split the corpus into training and evaluation set.  \n",
    "Stage 1 performs preprocessing and feature extraction.  \n",
    "Stage 2 calculates the statistics of extracted features.  \n",
    "Stage 3 performs noise weighting for training waveforms.  \n",
    "Stage 4 trains WaveNet using extracted features and noise weighted waveforms.  \n",
    "Stage 5 decodes waveform of evaluation data using trained WaveNet.  \n",
    "And finaly, Stage 6 perform noise shaping for generated waveforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If run `run.sh`, all of stages will be performed.\n",
    "\n",
    "But we can specify the stage to run with `--stage` options.\n",
    "\n",
    "- `run.sh --stage 0`: Run only the stage 0\n",
    "- `run.sh --stage 012`: Run the stages 0, 1, and 2.\n",
    "\n",
    "Here, let us run each stage step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If you run the main script run.sh, all of the above stages will be performed.  \n",
    "But we can specify the stage to be running with --stage option like this.  \n",
    "Here, to understand each stage, let us run each stage step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 0: Data preparation\n",
    "\n",
    "This stage performs download of corpus and list preparation.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/stage_0.png width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "First, stage 0, data prepration.  \n",
    "This stage downloads corpus from cloud and make the list of utterances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In arctic, there are seven speakers.  \n",
    "Here let us use `slt` to build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# you can specify the speaker via --spk (default=slt)\n",
    "!./run.sh --stage 0 --spk slt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In arctic, seven speakers exists.  \n",
    "Here let us use female speaker slt to build a model.  \n",
    "You can specify the speaker with `--spk` option.  \n",
    "Please run the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Corpus is saved in\n",
    "- `downloads/cmu_us_<spk_name>_arctic_mini`\n",
    "\n",
    "Two lists of wav files are created.\n",
    "- `data/tr_slt/wav.scp`: wav list file for training\n",
    "- `data/ev_slt/wav.scp`: wav list file for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "!tree -L 3 -I local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let me check the file. Please run the cell.  \n",
    "The downloaded corpus is saved in downloads/cmu_us_.  \n",
    "And 2 lists of wav files are created under the data directory.  \n",
    "data/tr_slt/wav.scp is for training and the other is for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The list file is that:\n",
    "- Each line has the path of wav file\n",
    "- All of the lines are sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    " !head -n 3 data/*_slt/wav.scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use 32 utts for training, 4 for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l < data/tr_slt/wav.scp\n",
    "!wc -l < data/ev_slt/wav.scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let me check the list file format. Please run the cell.  \n",
    "You can see that each line has the path of wav file and all of the lines are sorted.  \n",
    "Let me check the number of lines via wc commnd.  \n",
    "Here we use 32 utterances for training and 4 utterances for evalaution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# (Optional) here you can check the file with your commands!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 1: Feature extraction\n",
    "\n",
    "This stage performs feature extraction with the\n",
    "list file.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/stage_1.png width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Next, stage 1 feature extration.  \n",
    "This stage applies high pass filter as preprocessing and then extract feature vector using world analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters related to stage 1\n",
    "!head -n 36 run.sh | tail -n 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "At first, let me check the hyperparameters related to feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 1 with default settings\n",
    "!./run.sh --stage 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Please run the stage 1.  \n",
    "In the log, they show the hyperparameters and progress.  \n",
    "And this stage is performed in parallel using the multiprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hyperparameters can be changed via command line.  \n",
    "But it will overwrite the existing ones. Be careful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# example of changing hyperparameters of feature extraction\n",
    "# !./run.sh --stage 1 --mcep_dim 30 --shiftms 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If you want to change the hyperparameters of feature extraction, we can change by specifiying the option from command line.  \n",
    "But the feature vector will be overwritten, please be careful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Extracted features are saved as `hdf5` in\n",
    "- `hdf5/tr_slt/*.h5`: Feature file of training data \n",
    "- `hdf5/ev_slt/*.h5`: Feature file of evaluation data\n",
    "\n",
    "Lists of feature files are created \n",
    "- `data/tr_slt/feats.scp`\n",
    "- `data/ev_slt/feats.scp`\n",
    "\n",
    "High pass filtered training wav files are saved in\n",
    "- `wav_hpf/tr_slt/*.wav`: Filtered wav file of training data\n",
    "\n",
    "List of filetered wav files is created\n",
    "- `data/tr_slt/wav_hpf.scp`: List of filtered wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!tree -L 3 -I \"*.f0|local|cmu_*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "OK. Let me check the created files.  \n",
    "Extracted features are saved in the hdf5 format under hdf5 directory.  \n",
    "hdf5/tr_slt includes feature files for training and hdf5/ev_lst includes oens for evaluation.  \n",
    "Also lists of these feature vector are created under data directory.\n",
    "data/tr_slt/feats.scp and data/ev_slt/feats.scp.\n",
    "\n",
    "And for training data, high pass filetered waveform is saved uder wav_hpf directory.  \n",
    "The list of filtered wav files is also created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us check the list file format:\n",
    "- Each line has the path of feature or wav\n",
    "file  \n",
    "- All of the lines are sorted\n",
    "- Assume that all of the lists has the same\n",
    "order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [],
   "source": [
    "!head -n 3 data/*_slt/feats.scp\n",
    "!echo \"\"\n",
    "!head -n 3 data/tr_slt/wav_hpf.scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let me check the format of list files.  \n",
    "As the same as wav.scp files, each line has the path of feature or wav file, and all of the lines are sorted.  \n",
    "We assume that all of the lists has the same order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "hdf5 format can be loaded as `numpy.ndarray` in python using `h5py` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "17"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File(\"hdf5/tr_slt/arctic_a0001.h5\") as f:\n",
    "    print(f.keys())\n",
    "    feat = f[\"world\"][()]\n",
    "# or you can use our utils\n",
    "from wavenet_vocoder.utils import read_hdf5\n",
    "feat = read_hdf5(\"hdf5/tr_slt/arctic_a0001.h5\", \"world\")\n",
    "print(\"Feature shape: (#num_frames=%d, #feature_dims=%d)\" % (feat.shape[0], feat.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The extracted hdf5 file can be loaded as numpy.ndarray using h5py.   \n",
    "Here we load the feature via h5py.  \n",
    "Also we provide the utilities tool read_hdf5.  \n",
    "You can use this instead of h5py library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The feature is extracted with World.\n",
    "- `U/V binary` (1 dim)\n",
    "- `continuous F0` (1 dim), \n",
    "- `mcep`(25 dim = `mcep_dim + 1`) \n",
    "- `ap` (1 dim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(feat[:, 0])\n",
    "plt.title(\"U/V binary\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(feat[:, 1])\n",
    "plt.title(\"Continuous F0\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(feat[:, 2:26].T, aspect=\"auto\")\n",
    "plt.title(\"Mel-cepstrum\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(feat[:, -1])\n",
    "plt.title(\"Aperiodicity\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The dimenstion of feature vector is 28.  \n",
    "Let me check each components of feature vector.  \n",
    "The feature is extracted with World analyzer.  \n",
    "Let me visualize each features.\n",
    "U/V binary, contious F0, mel-ceptrum, and aperiodicity components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# (Optional) here you can check the file with your commands!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 2: Statistics calculation\n",
    "\n",
    "This stage calculates the mean and variance of features.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/stage_2.png width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Next, stage 2 statistics calculation.  \n",
    "This stage calculate the mean and variacne of features for feature normalization and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 2 with default settings\n",
    "!./run.sh --stage 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Please run the stage 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Calculated statistics are saved as `hdf5` format in\n",
    "- `data/tr_slt/stats.h5`\n",
    "\n",
    "`stats.h5` is used for:\n",
    "- Feature normalization during training\n",
    "- Calculation of noise shaping filter coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    }
   },
   "outputs": [],
   "source": [
    "!tree -L 3 -I \"*.f0|*.wav|*[0-9].h5|local|cmu_*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Through the stage 2, statistics file is saved as hdf5 format under the data/tr_slt.  \n",
    "Let me check the files.  \n",
    "`stats.h5` is the statistics of the training data.  \n",
    "`stats.h5` is used for feature normalization during training and calculation of noise shaping coefficient.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`stats.h5` can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "21"
    }
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"data/tr_slt/stats.h5\") as f:\n",
    "    print(f.keys())\n",
    "    print(f['world'].keys())\n",
    "    mean = f['world']['mean'][()]\n",
    "    scale = f['world']['scale'][()]\n",
    "    print(mean.shape)\n",
    "    print(scale.shape)\n",
    "    \n",
    "# or you use our utils\n",
    "mean = read_hdf5(\"data/tr_slt/stats.h5\", \"world/mean\")\n",
    "scale = read_hdf5(\"data/tr_slt/stats.h5\", \"world/scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let me load the statistics file.  \n",
    "It contains mean and scale, which is a variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# here you can check the file with your commands!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 3: Noise weighting\n",
    "\n",
    "This stage applies noise weighting filter to training\n",
    "wav files.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/stage_3.png width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Next, stage 3, noise weighting.  \n",
    "This stage applies noise weighting filter to high pass filtered wav files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters related to stage 3\n",
    "!head -n 38 run.sh | tail -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let me check the hyperparameters related to stage 3.  \n",
    "The hyperparametres are onlly use_noise_shaping which decides whether to perform noise shapng, \n",
    "and mag, which is the strength value from 0.0 to 1.0.  \n",
    "Experimentaly around 0.5 is a good for the best quality.  \n",
    "You can see the detail investigation in Dr. Tachibana's paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "22"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 3 with default settings\n",
    "!./run.sh --stage 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `use_noise_shaping=false`, `stage 3` will be skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Please run the stage 3.  \n",
    "Note that if you specify use_noise_shaping = false, this stage will be skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Noise weighting filtered wav files are saved in\n",
    "- `wav_nwf/tr_slt/*.wav`\n",
    "\n",
    "The list of noise weighting filtered wav files is saved as\n",
    "- `data/tr_slt/wav_nwf.scp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "23"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!tree -L 3 -I \"*.f0|*[0-9].h5|local|cmu_*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let me chech the created files.   \n",
    "Noise weighting filtered wav files are saved in wav_nwf/tr_slt.  \n",
    "The list of noise weighting filtered wav files is saved as data/tr_slt/wav_nwf.scp  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us check the difference of waveform here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "25"
    }
   },
   "outputs": [],
   "source": [
    "# listen to the samples\n",
    "import IPython.display\n",
    "IPython.display.display(IPython.display.Audio(\"wav_hpf/tr_slt/arctic_a0001.wav\"))\n",
    "IPython.display.display(IPython.display.Audio(\"wav_nwf/tr_slt/arctic_a0001.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "26"
    }
   },
   "outputs": [],
   "source": [
    "# show spectrogram\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "x, fs = sf.read(\"wav_hpf/tr_slt/arctic_a0001.wav\")\n",
    "x_ns, fs = sf.read(\"wav_nwf/tr_slt/arctic_a0001.wav\")\n",
    "plt.figure(figsize=(16, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.specgram(x, Fs=fs)\n",
    "plt.title(\"Original spectrogram\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.specgram(x_ns, Fs=fs)\n",
    "plt.title(\"Noise weighting filtered spectrogram\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here, let me check the difference.  \n",
    "Please run the cell.  \n",
    "Maybe you can understand the difference between them.  \n",
    "Let me also check the spectrogram. Please run the cell.\n",
    "You can confirm that the power in highfreqency became bigger and that in low frequency did smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Filtering related parameters `mlas/coef` and `mlsa/alpha` are added in `data/tr_slt/stats.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "27"
    }
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"data/tr_slt/stats.h5\") as f:\n",
    "    print(f.keys())\n",
    "    print(f[\"mlsa\"].keys())\n",
    "    print(f[\"mlsa\"][\"alpha\"])\n",
    "    print(f[\"mlsa\"][\"coef\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mlsa/coef` is the coefficient of MLSA filter, which is calculated from averaged mel-cepstrum and `mag`.  \n",
    "`mlsa/alpha` is the hyperparameter `alpha`, all pass filter coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "And through this stage, additional parameters are written in the statistics file `stats.h5`.  \n",
    "Let me check added parameters.  \n",
    "One is the mlsa/coef, which is the coefficient of MLSA fileter.  \n",
    "They are calculated from averated mel-ceptrum and mag parameters, as you saw in the Dr. Toda's lecture.  \n",
    "The other is the mlsa/alpha, which the all pass filter coefficient.  \n",
    "These parameters are used in the final stage, noise shaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# (Optional) here you can check the file with your commands!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 4: WaveNet training\n",
    "\n",
    "This stage trains WaveNet using extracted\n",
    "features and noise weighting filtered wav files.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/stage_4.png width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "I'm sorry I kept you waiting.  Now ready to train the network!  \n",
    "Next is the stage 4, WaveNet training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters related to stage 4\n",
    "!head -n 59 run.sh | tail -n 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "First, please run the cell to check the hyperparameters.  \n",
    "There are several parameters related to network structure, and the others are realted to training.  \n",
    "I will explain in later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "28"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 4 with default settings\n",
    "!./run.sh --stage 4 --iters 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Please run the stage 4.  Here we train 500 iteration by specifying the hyperparameters iters=500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Default network structure in `egs/arctic/sd-mini`.\n",
    "<div align=\"center\">\n",
    "    <img src=figs/wavenet.png width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "During the training, I will explain the relationship between the hyperparameters and network structure.  \n",
    "This figure shows the wavenet structure.  \n",
    "The hyperparameters related to network structure are \n",
    "- n_aux, which is the number of auxiliary features\n",
    "- n_resch. which is the number of channel in convoluional layer placed in residual blocks,\n",
    "- n_skipch, which is the number of channles in convolutional layer connected to output layers.\n",
    "- n_quantize, which is the number of classes of discretized wavenform,\n",
    "In my implementation, we use one-hot vector formated waveform as a inputs.\n",
    "\n",
    "Maybe you can understand the relationship between the hyperparameters and the structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example when `dilation_depth=3` and `dilation_repeat=2`.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/structure_ex.png width=45%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Remaining hyperparameters related to the structure are dilation_depth and dilation_repeat.  \n",
    "I will explain these parameters using the example.  \n",
    "This figure shows the example of diation_depth=3 and dilation_repeat=2.  \n",
    "The number of residual blocks is equal to dilation depth times dilation repeat, in this case the number of blocks is 6.  \n",
    "Each of dilation in each residual block starts from 2 to the power of 0, 2 to the power of 1, 2 to the power 2 and then repeat them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Make a batch by split a waveform into pieces.\n",
    "- `batch_size`: Number of batches\n",
    "- `batch_length`: Length of each batch\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/batch.png width=65%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Next, I will explain how to make the batch.\n",
    "This figure shows the overview of batch making.    \n",
    "The batch will be created in the windowing manner.\n",
    "batch_length decides the length of each piece and batch_size decides the number of pieces.  \n",
    "The length of each piece is batch_length + receptive field size and the next batch will be created with receptive_filed size overlap.  \n",
    "This is because we do not calculate the loss for corresponding to within receptive fields.  \n",
    "OK. Maybe you finished the training. Let me check the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model parameters are saved as  \n",
    "- `exp/tr_arctic_16k_sd_world_slt_*/checkpoint-*.pkl` \n",
    "\n",
    "Modle configuration is saved as  \n",
    "- `exp/tr_arctic_16k_sd_world_slt_*/model.conf`\n",
    "\n",
    "The directory name is automatically set to be unique depending on hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "29"
    }
   },
   "outputs": [],
   "source": [
    "!tree -L 3 -I \"*.f0|*.wav|*[0-9].h5|*.scp|*.log|local|cmu_*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let us check the files. please run the cell.  \n",
    "Model parameters are saved in exp/tr_buraburabura/checkpoint.pkl  and model configuration file is saved as model.conf in the same directory.  \n",
    "The name of the directory is automatically set o be unique depending on hyperparameters.  \n",
    "So if you change the hyperparemeters, the model will be saved in the different directory,  \n",
    "You do not need to care about the overwriting.  \n",
    "The log is also saved in log directory in the same directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model configuration file can be loaded as `argparse.Namespace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "30"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "conf = torch.load(\"exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/model.conf\")\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here, let me load the model.conf in python.  \n",
    "This file can be loaded using pytorch load function and then loaded as Namespace like this.  \n",
    "It contains the hyperparemters to build the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model parameters `checkpoint-*.pkl` can be loaded as `dict` which contains\n",
    "following information:\n",
    "- `iterations`: Number of iterations of this parameters\n",
    "- `optimizer`: `Dict` of states of optimizer\n",
    "- `model`: `OrderedDict` of Model\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "31"
    }
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/checkpoint-500.pkl\")\n",
    "print(state_dict.keys())\n",
    "print(state_dict[\"iterations\"])\n",
    "print(state_dict[\"optimizer\"].keys())\n",
    "print(state_dict[\"model\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let me load the model parameter file checkpoint.pkl. Please run the cell.  \n",
    "This file contains the number of iteration, optimizer states, and model parameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can resume training from `checkpoint-*.pkl` file with `--resume` options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "32"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!./run.sh --stage 4 \\\n",
    "    --iters 1000 \\\n",
    "    --resume exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/checkpoint-500.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So you can resume the training from checkpoint file.  \n",
    "Here, let us resume the training from 500 iters checkpoint and then continue to train until 1000 iters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can train using multi-gpu with `--n_gpus` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In colab, we can use only a single gpu :(\n",
    "# batch_size must be >= n_gpus\n",
    "# !./run.sh --stage 4 --n_gpus 2 --batch_size 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Also, if you want to use multi gpu, please use n_gpus options.  \n",
    "Note that please set batch_size to be greater than or equal to n_gpus in the case of multi gpus.  \n",
    "Unfortunately, we cannot use multi gpu so skipped it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# here you can check the file with your commands!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 5: WaveNet decoding\n",
    "\n",
    "This stage performs decoding of evaluation data.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/stage_5.png width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Next, stage 5, WaveNet training.  \n",
    "This stage performs decoding using evaluation set features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters related to stage 5\n",
    "!head -n 69 run.sh | tail -n 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let me check the hyperparameters related to stage 5.  \n",
    "Please run the cell.  \n",
    "\n",
    "(After running training)  \n",
    "There are some hyperparameters to specifying the model to be used.  \n",
    "If you not provide these hyperparameters, the script will use checkpoint-final.pkl corresponding to specfied hyperparameters.  \n",
    "So the important one is the decode_batch_size.  \n",
    "In this implementation, decoding is performed in batch mode, it can reduce the generation time.  \n",
    "But it requires GPU memory. So if you want to generated very long utterances, please be careful to avoid out of memory error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "35"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 5 with default setting\n",
    "!./run.sh --stage 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Before the explanation of hyperparameters, let us run the stage 5.  \n",
    "It takes time. please keep waiting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can specify the `checkpoint-*.pkl` file used for decoding and directory to\n",
    "be saved via `--checkpoint` and `--outdir` options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes times, comment out\n",
    "# !./run.sh --stage 5 \\\n",
    "#     --checkpoint exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/checkpoint-100.pkl\n",
    "#     --outdir exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/wav_ckpt_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For example, you can specify the model parameter file to be used for decoding like this.  \n",
    "It takes time, we skip it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use multi-gpu decoding via `--n_gpus` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In colab, we can use only a single gpu :(\n",
    "# !./run.sh --stage 5 --n_gpus 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Also, we can perform multi-gpu decoding.  you can use --n_gpus options like this but does not work in colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MOS on Naturalness\n",
    "\n",
    "<img src=https://camo.githubusercontent.com/b1b752447f3d040901fae34fefca03fc0765d187/68747470733a2f2f6b616e2d626179617368692e6769746875622e696f2f576176654e6574566f636f64657253616d706c65732f696d616765732f6d6f735f6e756d5f747261696e2e626d70 width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This figure shows the result of MOS on naturalness.  \n",
    "Vertical axis represents MOS on naturalness and horizon one represents the type of model.  \n",
    "The number in the brackets represents the number of utterances of training data.  \n",
    "From this figure, we can build a good sd wavenet vocoder if we parepare greater than 300 uttrances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Generated wav files are saved in \n",
    "- `exp/tr_arctic_sd_tr_arctic_16k_sd_*/wav`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "36"
    }
   },
   "outputs": [],
   "source": [
    "!tree exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "OK. I hope we finsh the decoding.  \n",
    "Let me check the generated wav files. please run the cell.  \n",
    "Generated wav files are saved in the wav directory located in the model directory.  \n",
    "These wav files are based on the noise weighting filtered one, so we need to restore to original waveform scale.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# (Optional) here you can check the file with your commands!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 6: Noise shaping\n",
    "\n",
    "This stage applies noise shaping filter to generated wav files.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=figs/stage_6.png width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "OK. The next stage is the final.  \n",
    "Stage 6, noise shaping.  \n",
    "This stage applies noise shaping filter, which is the inverse filter of noise weighting filter, to generated wav files to restore them into original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "37"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 6 with default setting\n",
    "!./run.sh --stage 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Please run the stage 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Restored wav files are saved in\n",
    "\n",
    "- `exp/tr_arctic_sd_tr_arctic_16k_sd_*/wav_nsf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "38"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!tree exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Restored wav fiels are saved in wav_nsf directory located in the model directory.  \n",
    "Let me check the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# (Optional) here you can check the file with your commands!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finished! Unfortunately, generated samples are just-like a noise.  \n",
    "So Let us check the samples which trained with `egs/arctic/sd` from  \n",
    "https://kan-bayashi.github.io/WaveNetVocoderSamples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Use pretrained model as vocoder\n",
    "\n",
    "Here we show how-to-use pretrained model as\n",
    "vocoder.  \n",
    "What we need to prepare is following three files:\n",
    "\n",
    "- `model.conf`:\n",
    "Model configuration file.\n",
    "- `checkpoint-*.pkl`: Model parameter file.\n",
    "- `stats.h5`: Statistics file.\n",
    "\n",
    "Let us pack following files into\n",
    "`pretrained_model/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "39"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# summarize trained model in the directory\n",
    "!mkdir pretrained_model\n",
    "!cp -v exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/stats.h5 \\\n",
    "    exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/model.conf \\\n",
    "    exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/checkpoint-1000.pkl \\\n",
    "    pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First, please prepare the list file of feature files to be decoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "40"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# here make a dummy features and the stored as hdf5 with key \"/world\"\n",
    "os.makedirs(\"dummy\", exist_ok=True)\n",
    "for idx, n_frames in enumerate([10, 20, 30, 40]): \n",
    "    x = np.random.randn(n_frames, 28)  # (#num_frames, #feature_dims)\n",
    "    with h5py.File(\"dummy/dummy_%d.h5\" % idx, \"w\") as f:\n",
    "        f[\"world\"] = x\n",
    "\n",
    "# make a list of features to be decoded.\n",
    "!find dummy -name \"*.h5\" > dummy_feats.scp\n",
    "\n",
    "# check\n",
    "!cat dummy_feats.scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Run the `--stage 56` by specifying `--feats` in the recipe directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "41"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# decode with pretrained model through the recipe\n",
    "!./run.sh --stage 56 \\\n",
    "    --outdir dummy_feats_wav \\\n",
    "    --feats dummy_feats.scp \\\n",
    "    --checkpoint pretrained_model/checkpoint-1000.pkl\n",
    "!ls dummy_feats_wav*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If you want to use outside of the recipe, directly call python scripts stored in\n",
    "`wavenet_vocoder/bin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "42"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# decode with pretrained model\n",
    "!python ../../../wavenet_vocoder/bin/decode.py \\\n",
    "     --feats dummy_feats.scp \\\n",
    "     --outdir dummy_feats_wav_2 \\\n",
    "     --checkpoint pretrained_model/checkpoint-1000.pkl \\\n",
    "     --fs 16000 \\\n",
    "     --n_gpus 1 \\\n",
    "     --batch_size 4\n",
    "# make list of wav files to be filtered\n",
    "!find dummy_feats_wav_2 -name \"*.wav\" > dummy_feats_wav_2/wav.scp\n",
    "# apply noise shaping filter\n",
    "!python ../../../wavenet_vocoder/bin/noise_shaping.py \\\n",
    "     --waveforms dummy_feats_wav_2/wav.scp \\\n",
    "     --outdir dummy_feats_wav_2_nsf \\\n",
    "     --stats pretrained_model/stats.h5 \\\n",
    "     --fs 16000 \\\n",
    "     --shiftms 5\n",
    "!ls dummy_feats_wav_2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combine with Sprocket\n",
    "\n",
    "Let us show how-to-combine wavenet vocoder with voice conversion toolkit [sprocket](https://github.com/k2kobayashi/sprocket).    \n",
    "Here, we generate converted voice with pretrained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Oh time is limited, we skip the usage of pretrained model part.  \n",
    "Let's move on the final part, combine with sprocket.  \n",
    "This part is corresponding to the final recipe 3, explain Dr. Kobayashi slides.\n",
    "\n",
    "In this part, we show how to combine wavenet vocoder with voice conversion toolkit sprocket.  \n",
    "We try to generated voice with pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changed directory\n",
    "!mkdir ../../../../conversion_example\n",
    "os.chdir(\"../../../../conversion_example\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "First, let us move on the directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, download pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# download sprocket model\n",
    "!../PytorchWaveNetVocoder/wavenet_vocoder/utils/download_from_google_drive.sh \\\n",
    "    \"https://drive.google.com/open?id=1PiGDyYDQt0b4h6KAV1MOmDxHjHUv1cT6\" \\\n",
    "    downloads/sprocket_pretrained\n",
    "\n",
    "# download wavenet vocoder model\n",
    "!../PytorchWaveNetVocoder/wavenet_vocoder/utils/download_from_google_drive.sh \\\n",
    "    \"https://drive.google.com/open?id=1AhtRB0vTkjDrum-dfgaiXnQgsAAiYMGW\" \\\n",
    "    downloads/wavenet_vocoder_pretrained\n",
    "\n",
    "# download wav samples\n",
    "!../PytorchWaveNetVocoder/wavenet_vocoder/utils/download_from_google_drive.sh \\\n",
    "    \"https://drive.google.com/open?id=1kBwF7ejyCR5aI9FitmMSCnWdPCNVouqg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here let us download sprocket pretrained model, and wavenet vocoder pretrained model, and sample speech for the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Sprocket pretrained model\n",
    "    - `GMM_mcep.pkl`: GMM param file for mcep conversion.\n",
    "    - `<src_spk>.yml`: Source speaker yaml file.\n",
    "    - `<src_spk>-<tar_spk>.yml`: Source-target speaker pair yaml file.\n",
    "    - `<src_spk>.h5`: Statistics file of source speaker.\n",
    "    - `<tar_spk>.h5`: Statistics file of target speaker.\n",
    "    - `cvgv.h5`: Statistics file of global variance for converted features.\n",
    "    \n",
    "- Target speaker WaveNet vocoder pretrained model\n",
    "    - `model.conf`: Model configuration file.\n",
    "    - `checkpoint-*.pkl`: Model parameter file.\n",
    "    - `stats.h5`: Statistics file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls downloads/*pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let me check the downloaded pretrained models.  \n",
    "Sprocket model includes ...  \n",
    "And wavenet vocoder model includes ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, extract features and then convert them to target speaker's one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "![ ! -e hdf5 ] && mkdir hdf5\n",
    "![ ! -e wav ] && mkdir wav\n",
    "!PYTHONPATH=../sprocket/example/src \\\n",
    "    python ../sprocket/sprocket/bin/convert_feats.py \\\n",
    "        --cvmcep0th True \\\n",
    "        --cvcodeap True \\\n",
    "        --cvgvstats downloads/sprocket_pretrained/cvgv.h5 \\\n",
    "        --org_yml downloads/sprocket_pretrained/rms.yml \\\n",
    "        --pair_yml downloads/sprocket_pretrained/rms-slt.yml \\\n",
    "        --org_stats downloads/sprocket_pretrained/rms.h5 \\\n",
    "        --tar_stats downloads/sprocket_pretrained/slt.h5 \\\n",
    "        --mcepgmmf downloads/sprocket_pretrained/GMM_mcep.pkl \\\n",
    "        --iwav downloads/samples/src/arctic_b0536.wav \\\n",
    "        --cvfeats hdf5/arctic_b0536.h5 \\\n",
    "        --owav wav/arctic_b0536.wav\n",
    "!ls hdf5 wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Next, extract features and then convert them to target speaker's one.  \n",
    "Input waveform is specified by `--iwav` and then converted feats are saved as `--cvfeats`, vocoded waveform is saved as `--owav`.\n",
    "Converted feature vector is saved in hdf5 directory, and converted voice with vocoder is saved in wav."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then generate waveform with pretrained wavenet using converted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: require too much time.\n",
    "# decode with wavenet vocoder\n",
    "!find hdf5 -name \"*.h5\" > hdf5/feats.scp\n",
    "!python ../PytorchWaveNetVocoder/wavenet_vocoder/bin/decode.py \\\n",
    "     --feats hdf5/feats.scp \\\n",
    "     --outdir wav_wnv \\\n",
    "     --checkpoint downloads/wavenet_vocoder_pretrained/checkpoint-final.pkl \\\n",
    "     --fs 16000 \\\n",
    "     --n_gpus 1 \\\n",
    "     --batch_size 4\n",
    "# apply noise shaping filter\n",
    "!find wav_wnv -name \"*.wav\" > wav_wnv/wav.scp\n",
    "!python ../PytorchWaveNetVocoder/wavenet_vocoder/bin/noise_shaping.py \\\n",
    "     --waveforms wav_wnv/wav.scp \\\n",
    "     --outdir wav_wnv_nsf \\\n",
    "     --stats downloads/wavenet_vocoder_pretrained/stats.h5 \\\n",
    "     --fs 16000 \\\n",
    "     --shiftms 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "And then let us generate with wavenet vocoder!  \n",
    "You can run this cell but it takes time.  \n",
    "So we have put pre-generated ones in `samples`.  \n",
    "Let us listen to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# listen to pre-synthesized ones\n",
    "import IPython.display\n",
    "print(\"Source\")\n",
    "IPython.display.display(IPython.display.Audio(\"downloads/samples/src/arctic_b0536.wav\"))\n",
    "print(\"Target\")\n",
    "IPython.display.display(IPython.display.Audio(\"downloads/samples/tar/arctic_b0536.wav\"))\n",
    "print(\"Converted voice with vocoder\")\n",
    "IPython.display.display(IPython.display.Audio(\"downloads/samples/vocoder/arctic_b0536.wav\"))\n",
    "print(\"Converted voice with wavenet vocoder\")\n",
    "IPython.display.display(IPython.display.Audio(\"downloads/samples/wavenet_vocoder/arctic_b0536.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "I will play source voice, target  voice, converted voice with vocoder, and that with wavenet vocoder.  \n",
    "Maybe you can understand the difference with headphone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(\"running time = %s minite\" % ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "- Introduced voice conversion with direct waveform modeling\n",
    "- Introduced Sprocket /  PytorchWaveNetVocoder\n",
    "    - Can build GMM-based VC / DIFFVC  & WaveNet vocoder\n",
    "    - Can combine both module to generate high quality converted voices\n",
    "\n",
    "Thank you for your attendance!  \n",
    "If you have time, please send us feedback via [Google form](https://forms.gle/28QrvGRBAAiKpWas8). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "That's it. I conclude our tutorial.  \n",
    "In this tutorial, we introduced voice conversion with direct waveform modeling and introduce sprocket and pytorch wavenet vocoder.  \n",
    "I hope that now you are ready to start the research of voice conversion!  \n",
    "Thank you for you attendance! if you have time please send us feedback via google form!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "wavenet_vocoder.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
